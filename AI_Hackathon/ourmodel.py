# -*- coding: utf-8 -*-
"""ourmodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Z7eLaOHX9D65r-yNEoVgNFnr5od3KEh
"""

# pip install pdfplumber

from transformers import AutoModel, AutoTokenizer
from nltk.tokenize import sent_tokenize
import pdfplumber
import os
import re

# Load pre-trained model and tokenizer (same as before)
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

# Function to preprocess text (same as before)
def preprocess_text(text):
  """
  Preprocesses text to improve tokenization:
    - Removes extra whitespace and newlines
    - Converts to lowercase
    - Handles potential special characters (optional, customize based on needs)
  """
  text = re.sub(r'\s+', ' ', text)  # Remove extra whitespace and newlines
  text = text.strip()
  text = text.lower()  # Convert to lowercase
  # Optionally add more specific cleaning steps for special characters if needed
  return text
a_tokens=[]
# Function to tokenize sentences from text (modified)
def tokenize_sentences(text):
  """
  Extracts and tokenizes sentences from the preprocessed text.
  """
  sentences = sent_tokenize(preprocess_text(text))
  tokens = []
  for sentence in sentences:
    # Use tokenizer for each sentence, ensuring it's a string
    encoded_sentence = tokenizer(sentence, return_tensors="pt")
    a_tokens.append(encoded_sentence)  # Append encoded sentence (tensor)
  return a_tokens
import os
# Function to convert PDF to tokenized sentences (modified)
def convert_pdf_to_tokens(file_path):
  """
  Main function to convert a PDF file to tokenized sentences.
  """
  if not os.path.exists(file_path):
    raise ValueError(f"PDF file not found: ")

  try:
    with pdfplumber.open(file_path) as pdf:
      text = ""
      for page in pdf.pages:
        text += page.extract_text()
  except Exception as e:
    raise ValueError(f"Error extracting text from PDF: {e}")

  return tokenize_sentences(text)

# Download NLTK punkt package (same as before)
import nltk
nltk.download('punkt')
a_tokens=[]
# Process the PDF (assuming you have the convert_pdf_to_tokens function)
try:
    pdf_file = "/content/augmented_synonymsssssss.pdf"  # Replace with your PDF path
    converting = convert_pdf_to_tokens(file_path=pdf_file)

    print("Sentence tokens (encoded):", a_tokens)  # Print encoded sentences
except Exception as e:
    print("An error occurred:", e)
except ValueError as e:
  print(f"Error: {e}")

# Example sentence for reference (no change)


# Loop through processed PDF sentences (modified)
encoded_document_sentences = []
for tokenized_sentence in a_tokens:
  # Assuming a_tokens contains encoded sentence tensors from convert_pdf_to_tokens
  encoded_document_sentences.append(tokenized_sentence)

#print("Encoded document sentences:", encoded_document_sentences)

import torch
sentence = input("Enter a Query :")
sentence = sentence.lower()  # Lowercase for consistency

# Encode the example sentence (no change)
encoded_sentence = tokenizer(sentence, return_tensors="pt")
with torch.no_grad():
  sentence_embedding = model(**encoded_sentence).pooler_output
  document_embeddings = [model(**doc_sent).pooler_output for doc_sent in encoded_document_sentences ]

  similarities = [torch.nn.functional.cosine_similarity(sentence_embedding, doc_embedding) for doc_embedding in document_embeddings]

"""**CONVERTING THE PDF INTO EMBEDDINGS**

**PASSING THE SCANNED DOCUMENT**

**PERFORMING ENCODINGS**

**CALCULATING SIMILARITY**

**CALCULATING SIMILAR WORDS**
"""

if len(similarities) == 0:
    print("The 'similarities' list is empty. Please ensure it contains data before proceeding.")
    exit()

import torch

most_similar_idx = torch.argmax(torch.tensor(similarities))
most_similar_text = a_tokens[most_similar_idx]
similarity_score = similarities[most_similar_idx]

print(f"Most similar sentence in document: {most_similar_text}")
print(f"Similarity score: {similarity_score}")

"""Integrating API"""

import requests

API_URL = "https://api-inference.huggingface.co/models/distilbert/distilbert-base-uncased-finetuned-sst-2-english"
headers = {"Authorization": "Bearer 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"}

def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
	"inputs": sentence,
})
output

if output[0][0]['label']=='POSITIVE' and similarity_score > 0.5:
  print('POSITIVE')
elif output[0][0]['label']=='NEGATIVE' and similarity_score>0.7:
  print('POSITVE')
elif output[0][0]['label']=='NEGATIVE' and similarity_score<0.7:
  print("NEGATIVE")

else:
  print('NEGATIVE')